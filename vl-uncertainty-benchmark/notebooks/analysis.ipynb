{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VL-Uncertainty-Benchmark Analysis\n",
    "\n",
    "This notebook provides interactive analysis of benchmark results.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.metrics import (\n",
    "    expected_calibration_error,\n",
    "    reliability_diagram,\n",
    "    plot_calibration_comparison,\n",
    "    compute_calibration_metrics,\n",
    ")\n",
    "from src.analysis import (\n",
    "    find_high_confidence_failures,\n",
    "    cluster_failures_by_degradation,\n",
    "    generate_failure_report,\n",
    "    plot_pareto_frontier,\n",
    "    plot_edge_vs_cloud_comparison,\n",
    "    compute_scaling_efficiency,\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results\n",
    "\n",
    "Update the path below to point to your benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to your results directory\n",
    "RESULTS_DIR = '../results/run_YYYYMMDD_HHMMSS'\n",
    "\n",
    "# Load summary and detailed results\n",
    "summary_df = pd.read_csv(f'{RESULTS_DIR}/summary.csv')\n",
    "results_df = pd.read_csv(f'{RESULTS_DIR}/all_results.csv')\n",
    "\n",
    "print(f\"Loaded {len(summary_df)} models, {len(results_df)} evaluations\")\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by uncertainty AUROC (higher is better)\n",
    "summary_sorted = summary_df.sort_values('uncertainty_auroc', ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# AUROC comparison\n",
    "colors = ['green' if t == 'edge' else 'blue' for t in summary_sorted['tier']]\n",
    "axes[0].barh(summary_sorted['model'], summary_sorted['uncertainty_auroc'], color=colors)\n",
    "axes[0].set_xlabel('Uncertainty AUROC')\n",
    "axes[0].set_title('Can Uncertainty Predict Errors?')\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', label='Random')\n",
    "\n",
    "# ECE comparison\n",
    "axes[1].barh(summary_sorted['model'], summary_sorted['calibrated_ece'], color=colors)\n",
    "axes[1].set_xlabel('ECE (lower is better)')\n",
    "axes[1].set_title('Calibration Error')\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[2].barh(summary_sorted['model'], summary_sorted['accuracy'], color=colors)\n",
    "axes[2].set_xlabel('Accuracy')\n",
    "axes[2].set_title('Overall Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pareto Analysis: Compute vs Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pareto frontier\n",
    "fig = plot_pareto_frontier(\n",
    "    summary_df,\n",
    "    x='params',\n",
    "    y='uncertainty_auroc',\n",
    "    model_col='model',\n",
    "    tier_col='tier',\n",
    "    title='Compute vs. Uncertainty Quality Tradeoff'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scaling efficiency\n",
    "scaling = compute_scaling_efficiency(summary_df, 'params', 'uncertainty_auroc')\n",
    "\n",
    "print(\"Scaling Analysis:\")\n",
    "print(f\"  Slope: {scaling['slope']:.4f}\")\n",
    "print(f\"  R²: {scaling['r_squared']:.3f}\")\n",
    "print(f\"  Interpretation: {scaling['interpretation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Edge vs Cloud Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare edge vs cloud within model families\n",
    "fig = plot_edge_vs_cloud_comparison(summary_df, metric_col='uncertainty_auroc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison\n",
    "edge_models = summary_df[summary_df['tier'] == 'edge']\n",
    "cloud_models = summary_df[summary_df['tier'] == 'cloud']\n",
    "\n",
    "print(\"Edge vs Cloud Statistics:\")\n",
    "print(f\"\\nEdge Models (n={len(edge_models)}):\")\n",
    "print(f\"  Mean AUROC: {edge_models['uncertainty_auroc'].mean():.3f}\")\n",
    "print(f\"  Mean ECE: {edge_models['calibrated_ece'].mean():.3f}\")\n",
    "print(f\"  Mean Accuracy: {edge_models['accuracy'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nCloud Models (n={len(cloud_models)}):\")\n",
    "print(f\"  Mean AUROC: {cloud_models['uncertainty_auroc'].mean():.3f}\")\n",
    "print(f\"  Mean ECE: {cloud_models['calibrated_ece'].mean():.3f}\")\n",
    "print(f\"  Mean Accuracy: {cloud_models['accuracy'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reliability Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few models to compare\n",
    "models_to_compare = summary_df['model'].head(4).tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, model_name in zip(axes, models_to_compare):\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    # Compute calibration curve\n",
    "    from src.metrics.calibration import calibration_curve\n",
    "    confs, accs, counts = calibration_curve(\n",
    "        model_df['calibrated_confidence'].values,\n",
    "        model_df['is_correct'].values\n",
    "    )\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect')\n",
    "    ax.bar(confs, accs, width=0.05, alpha=0.7)\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'{model_name}')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance by Degradation Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate performance by degradation\n",
    "deg_performance = results_df.groupby(['model', 'degradation_type']).agg({\n",
    "    'is_correct': 'mean',\n",
    "    'calibrated_confidence': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Pivot for heatmap\n",
    "pivot_acc = deg_performance.pivot(index='model', columns='degradation_type', values='is_correct')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_acc, annot=True, fmt='.2f', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "plt.title('Model Accuracy by Degradation Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance degradation with severity\n",
    "severity_perf = results_df.groupby(['model', 'severity']).agg({\n",
    "    'is_correct': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for model in summary_df['model'].unique()[:5]:  # Top 5 models\n",
    "    model_data = severity_perf[severity_perf['model'] == model]\n",
    "    plt.plot(model_data['severity'], model_data['is_correct'], 'o-', label=model)\n",
    "\n",
    "plt.xlabel('Degradation Severity')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Degradation Severity')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high-confidence failures\n",
    "failures = find_high_confidence_failures(\n",
    "    results_df,\n",
    "    confidence_threshold=0.9,\n",
    "    confidence_col='calibrated_confidence'\n",
    ")\n",
    "\n",
    "print(f\"Found {len(failures)} high-confidence failures (>90% confidence but wrong)\")\n",
    "print(f\"This is {len(failures)/len(results_df):.2%} of all predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster failures by degradation\n",
    "if len(failures) > 0:\n",
    "    clusters = cluster_failures_by_degradation(failures)\n",
    "    \n",
    "    print(\"\\nFailures by Degradation Type:\")\n",
    "    for deg_type, stats in clusters.items():\n",
    "        print(f\"  {deg_type}: {stats['count']} failures ({stats['percentage']:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full failure report\n",
    "if len(failures) > 0:\n",
    "    report = generate_failure_report(failures, total_samples=len(results_df))\n",
    "    \n",
    "    print(\"\\nFailure Report Summary:\")\n",
    "    print(f\"  Total failures: {report['summary']['total_failures']}\")\n",
    "    print(f\"  Failure rate: {report['summary']['failure_rate']:.2%}\")\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    for rec in report['recommendations']:\n",
    "        print(f\"  - {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Conclusions\n",
    "\n",
    "### Research Question: Does scaling up models give meaningfully better uncertainty calibration?\n",
    "\n",
    "Based on the analysis above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize findings\n",
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(edge_models) > 0 and len(cloud_models) > 0:\n",
    "    auroc_diff = cloud_models['uncertainty_auroc'].mean() - edge_models['uncertainty_auroc'].mean()\n",
    "    ece_diff = edge_models['calibrated_ece'].mean() - cloud_models['calibrated_ece'].mean()\n",
    "    \n",
    "    print(f\"\\n1. Uncertainty Quality (AUROC):\")\n",
    "    print(f\"   Edge avg: {edge_models['uncertainty_auroc'].mean():.3f}\")\n",
    "    print(f\"   Cloud avg: {cloud_models['uncertainty_auroc'].mean():.3f}\")\n",
    "    print(f\"   Difference: {auroc_diff:.3f}\")\n",
    "    \n",
    "    print(f\"\\n2. Calibration (ECE):\")\n",
    "    print(f\"   Edge avg: {edge_models['calibrated_ece'].mean():.3f}\")\n",
    "    print(f\"   Cloud avg: {cloud_models['calibrated_ece'].mean():.3f}\")\n",
    "    print(f\"   Difference: {ece_diff:.3f}\")\n",
    "    \n",
    "    print(f\"\\n3. Recommendation for Robotics Middleware:\")\n",
    "    if auroc_diff < 0.05 and ece_diff < 0.02:\n",
    "        print(\"   Edge models provide comparable uncertainty quality.\")\n",
    "        print(\"   They are likely SUFFICIENT for reactive→deliberative switching.\")\n",
    "    else:\n",
    "        print(\"   Cloud models provide meaningfully better uncertainty.\")\n",
    "        print(\"   Consider using cloud models for critical decisions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
